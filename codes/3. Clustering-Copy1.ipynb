{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04de922",
   "metadata": {},
   "source": [
    "# 3. Clustering\n",
    "- Word2Vec\n",
    "- greedy_modularity_communities 클러스터 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c531c17d",
   "metadata": {},
   "source": [
    "## 1. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355b8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ffae1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터 실행했을 경우에만 실행\n",
    "token_df = pd.read_csv('./Data/Preprocess_Data/word_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88fe2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복사본\n",
    "t = token_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09896ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year 칼럼의 뒤의 두 자리 추출\n",
    "t['year_suffix'] = t['year'].astype(str).str[-2:]\n",
    "# list 칼럼의 단어들을 분리하여 리스트로 변환\n",
    "word_lists = t['tokens'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296f70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 2267163/2267163 [07:46<00:00, 4859.48it/s]\n"
     ]
    }
   ],
   "source": [
    "new_word_lists = []\n",
    "for words, suffix in tqdm(zip(word_lists, t['year_suffix']), total=len(t)):\n",
    "    new_words = [str(suffix) + '_' + word for word in words]\n",
    "    new_word_lists.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf0c9b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 2267163/2267163 [00:26<00:00, 84862.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# 리스트를 하나의 문자열로 변환하여 list 칼럼에 덮어씌움\n",
    "t['tokens'] = pd.Series([' '.join(words) for words in tqdm(new_word_lists)])\n",
    "\n",
    "# year_suffix 칼럼 제거\n",
    "t = t.drop('year_suffix', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b358f94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13_method 13_peer_to_peer_streaming 13_video_o...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13_paper 13_discus 13_bacterial 13_network_com...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_article 13_treat 13_digital_humanity 13_wor...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_work 13_describes 13_preliminary 13_step 13...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13_goal 13_extraction 13_learning_by_demonstra...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  year\n",
       "0  13_method 13_peer_to_peer_streaming 13_video_o...  2013\n",
       "1  13_paper 13_discus 13_bacterial 13_network_com...  2013\n",
       "2  13_article 13_treat 13_digital_humanity 13_wor...  2013\n",
       "3  13_work 13_describes 13_preliminary 13_step 13...  2013\n",
       "4  13_goal 13_extraction 13_learning_by_demonstra...  2013"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42068c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv('Data/Preprocess_Data/word_token+year.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa91a42",
   "metadata": {},
   "source": [
    "### 연도 집어넣은 것을 바탕으로 word2vec 시행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ee9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('./Data/Preprocess_Data/word_token+year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7769cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13_method 13_peer_to_peer_streaming 13_video_o...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13_paper 13_discus 13_bacterial 13_network_com...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_article 13_treat 13_digital_humanity 13_wor...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_work 13_describes 13_preliminary 13_step 13...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13_goal 13_extraction 13_learning_by_demonstra...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  year\n",
       "0  13_method 13_peer_to_peer_streaming 13_video_o...  2013\n",
       "1  13_paper 13_discus 13_bacterial 13_network_com...  2013\n",
       "2  13_article 13_treat 13_digital_humanity 13_wor...  2013\n",
       "3  13_work 13_describes 13_preliminary 13_step 13...  2013\n",
       "4  13_goal 13_extraction 13_learning_by_demonstra...  2013"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72710d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gx/q7vn3yv17rvb0vy6chbbbztw0000gn/T/ipykernel_13254/3891386349.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 텍스트 데이터 전처리 및 토큰화, 불용어 제거\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/gx/q7vn3yv17rvb0vy6chbbbztw0000gn/T/ipykernel_13254/3891386349.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 텍스트 데이터 전처리 및 토큰화, 불용어 제거\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     return [\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\" \\1 \\2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\" \\1 \\2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 전처리 및 토큰화, 불용어 제거\n",
    "t['tokens2'] = t['tokens'].apply(lambda x: [word for word in word_tokenize(x)])\n",
    "sentences = t['tokens2'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef287cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54af6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# 단어 벡터 확인\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f01279f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('Data/Preprocess_Data/year_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24df77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 벡터 저장\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"Data/Preprocess_Data/year_word_vectors.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dea04c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('20_deep_learning_such', 0.9240278005599976), ('20_face_tracking', 0.922843337059021), ('20_self_calibration', 0.9179024696350098), ('20_batch_processing', 0.916452169418335), ('20_quantum_annealing', 0.9142048954963684), ('20_sw', 0.9131624698638916), ('20_tasnet', 0.9126613736152649), ('20_parallel_structure', 0.9123446345329285), ('20_biped', 0.9123086333274841), ('20_tsv', 0.9116710424423218), ('20_broadens', 0.9115983843803406), ('20_matrix_profile', 0.9092435240745544), ('20_duo', 0.9091187119483948), ('20_coral', 0.9090192317962646), ('20_semantic_slam', 0.9086829423904419), ('20_pea', 0.9064937233924866), ('20_graphic_processing_unit_in', 0.9060552716255188), ('20_hierarchal', 0.9059861302375793), ('20_defect_prediction', 0.9052213430404663), ('20_supercomputing_system', 0.9050830602645874), ('20_convolutional_neural_network_used', 0.9048026204109192), ('20_system_form', 0.903627872467041), ('20_deep_learning_on', 0.9034767150878906), ('20_tmp', 0.9033932089805603), ('20_qsm', 0.9033764600753784), ('20_speeded', 0.9033143520355225), ('20_calibrator', 0.903209388256073), ('20_machine_learning_component', 0.9031828045845032), ('20_lossy_compression', 0.9029059410095215), ('20_cubesat', 0.9028493165969849)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('20_deep_learning', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41b12972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4937513"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('13_machine_learning', '14_machine_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5d1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020년 데이터만\n",
    "t_2023 = t[t['year']==2023]\n",
    "t_2023.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_2023.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf380ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터 전처리 및 토큰화\n",
    "t_2023['tokens2'] = t_2023['tokens'].astype(str).apply(lambda x: word_tokenize(x))\n",
    "t_2023['tokens2'] = t_2023['tokens2'].apply(lambda tokens: [token for token in tokens if token != ','])\n",
    "\n",
    "\n",
    "sentences = t_2023['tokens2'].tolist()\n",
    "sentences = [sentence for sentence in tqdm(sentences, desc=\"Processing sentences\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b210540",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1144936424.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/gx/q7vn3yv17rvb0vy6chbbbztw0000gn/T/ipykernel_13254/1144936424.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sentences_flat에서 19년도 단어가 들어간 데이터만 찾기\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# sentences_flat에서 19년도 단어가 들어간 데이터만 찾기\n",
    "sentences_flat = [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 단어들에 대한 Word2Vec 결과 벡터 추출\n",
    "word_vecs = [word_vectors[word] for word in sentences_flat]\n",
    "\n",
    "# 모든 단어 쌍의 코사인 유사도 계산\n",
    "similarity_matrix = cosine_similarity(word_vecs)\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=sentences_flat, columns=btq_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508093cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사성 임계값 설정\n",
    "similarity_threshold = 0.13\n",
    "\n",
    "# 주어진 단어들에 대한 Word2Vec 결과 벡터 추출\n",
    "word_vecs = [word_vectors[word] for word in sentences_flat if word in word_vectors]\n",
    "\n",
    "# 각 커뮤니티의 중심점을 찾기\n",
    "centroids = []\n",
    "for community in communities:\n",
    "    community_vectors = [word_vectors[word] for word in community if word in word_vectors]\n",
    "    if len(community_vectors) > 0:\n",
    "        community_centroid = sum(community_vectors) / len(community_vectors)\n",
    "        centroids.append(community_centroid)\n",
    "\n",
    "# 커뮤니티 중심점을 기반으로 새 그래프 생성\n",
    "G_centroids = nx.Graph()\n",
    "\n",
    "# 중심점 노드 추가\n",
    "for i, centroid in enumerate(community_centroids):\n",
    "    G_centroids.add_node(centroid, label=f\"Community {i+1}\")\n",
    "\n",
    "# 코사인 유사도를 사용하여 중심점 간의 유사도 계산 및 연결\n",
    "G_centroids = nx.Graph()\n",
    "for i in range(len(centroids)):\n",
    "    for j in range(i+1, len(centroids)):\n",
    "        centroid_i = centroids[i]\n",
    "        centroid_j = centroids[j]\n",
    "        similarity = cosine_similarity([centroid_i], [centroid_j])[0][0]\n",
    "        if similarity > similarity_threshold:\n",
    "            G_centroids.add_edge(f\"Centroid {i+1}\", f\"Centroid {j+1}\", weight=similarity)\n",
    "\n",
    "# 그래프 그리기\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "node_labels = nx.get_node_attributes(G_centroids, 'label')\n",
    "nx.draw(G_centroids, labels=node_labels, ax=ax)\n",
    "plt.title('Paper Abstract Network with Communities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520f572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3f003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592c689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
